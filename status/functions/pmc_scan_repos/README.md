# pmc_scan_repos function app (readme)

## Summary

Scans all apt and yum repositories and is triggered by a timer once a day. Results are added to the results-queue which triggers a function in **pmc_status_delivery** to publish the results.

## Function details
### generate_repo
Scans [packages.microsoft.com](https://packages.microsoft.com) for a list of apt and yum repositories and adds them to repo-request-queue for them to be checked by **check_repo**. Apt repositories are added to to the queue with a "dist" field specifying one dist to check per **check_repo** function call. Filters of apt and yum repositories are also added to the results-queue to handle deleted repositories or apt dists. 

- **trigger**: Timer everyday at 8:30:00 am UTC (1:30 am PST)

- **inputs**: None

- **outputs**:

    repo-request-queue - Repository urls are added to this queue and check_repo will dequeue and check the status of the repo. Apt repositories in the queue have an additional "dist" field specifying 1 dist to check for the repository. 

    results-queue - Two filters are added to the results-queue, one for apt and another for yum repositories. A filter contains repository urls and dists so that should a repository or dist be removed, the status JSON will also have it removed. 

### check_repo
Checks apt and yum repositories in the repo-request-queue according to the repoaudit tool. Results are added to the results-queue. Apt repositories can optionally specify which dists should be checked. 

- **trigger**: Messages in the Azure storage queue *repo-request-queue*. 

- **inputs**: 

    inputblobpubkeys - JSON blob with a list of public key url's to use for signature verification. The content of the blob is located in 'static-data/pubkeys.json' in the storage account connection string pmcstatusprimary_CONNECTION.

- **outputs**: 
    
    results-queue - The status of each repository is added to the results-queue for publishing. 

## Function app Configuration
### - `host.json` 

We want repositories to be checked in parallel so the queue settings and scale out are configured for parallelism. 
- ***batchsize = 4*** so that 4 repositories are dequeued at a time and checked in parallel. 

- ***newBatchThreshold = 2*** so once there are only 2 repositories left being checked at a given time, another batchSize (4) will be dequeued. This means 6 repositories can be checked at a time by a single instance. 

- ***maxDequeueCount = 4*** means the function can fail 4 times until the message moved to a poison queue. This function is known to fail due to a bug with azure functions outlined below. 

- **visibilityTimeout = 00:10:00** so that should the function fail checking a repository, it will only retry that repository 10 minutes after it failed. 

### - Portal settings
**This function must be on a premium plan** since many repository checks take well over the 10 minute time limit imposed by the consumption plan. 

In Setting->Configuration->Application settings the following values must be configured.

- **AzureWebJobsStorage**: Storage account connection string the function requires to run. This is where temporary files and other function file are stored. Each function app should have its own storage account. 

- **pmcstatusprimary_CONNECTION**: pmcstatusprimary storage account connection string detailed in this [readme](../../README.md). This setting should be same across pmc_scan_repos, pmc_scan_mirrors, and pmc_status_delivery. 

Lastly, the **Scale Out** setting should be set as follows to allow for scale out when needed. 

- Maximum Burst: 30
- Minimum Instances: 1
- Always Ready Instances: 1
- Enforce Scale Out Limit: No

And **Scale up** setting can be set to Production EP1 instance with 210 ACU, 3.5 GB memory. 

## Dependency on Repoaudit command line tool

[Repoaudit](https://github.com/microsoft/linux-package-repositories) is a publicly available commandline tool on GitHub developed by the PMC team that checks for repository errors. It is also available as a python module on [PyPI](https://pypi.org/project/repoaudit/). Repository status generated by the Azure Function backend uses the repoaudit codebase as an imported module.

To use a custom version of the repoaudit tool, you have to deploy the function app with a custom local package. To do so first load the dependencies locally:

```
pip install  --target=".python_packages/lib/site-packages"  -r requirements.txt
```

Then edit the repoaudit module in `.python_packages/lib/site-packages/repoaudit/`.

Finally, deploy the function with the custom repoaudit module:

```
func azure functionapp publish <functionappname in Azure subscription> --python --no-build
```

The `--no-build` option will use the local versions of the modules in `.python_packages/`. 

## Known Issues
### - Certificate file disappears
It appears that the certificate provided by the container that runs the function code will disappear at high loads with many instances running. It is not known at the moment why this issue happens but [forums online](https://github.com/Azure/Azure-Functions/issues/1805) suggest that the filesystem is failing to mount. The current workaround is to create a certificate file in a temporary directory and use that for all requests. This appears to work better but the function continues to occasionally fail. Simply setting the maxDequeueCount to 4 is a suitable workaround that makes the function try checking the repository 4 times until it stops and moves the message to a poison queue. With this configuration, all repositories are successfully checked every day. 

