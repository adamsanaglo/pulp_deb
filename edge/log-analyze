#!/usr/bin/python3
import argparse
import gzip
import json
import os
import re
import subprocess
import time
from collections import Counter, defaultdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Pattern, Set, Tuple

show_verbose = False
VERSION = "1.1.0"


def verbose(*args, **kwargs):
    global show_verbose
    if show_verbose:
        print(*args, **kwargs)


def get_contents(logfile: str) -> List[str]:
    """
    Get the lines in logfile. If logfile ends in .gz, decompress it.
    """
    if ":" in logfile:
        host, logfile = logfile.split(":")
    else:
        host = ""

    if "/" not in logfile:
        logfile = f"/var/log/nginx/{logfile}"

    start = time.time()
    if host:
        verbose(f"Reading {logfile} from {host}...", end="", flush=True)
        result = get_remote_contents(logfile, host)
    else:
        verbose(f"Reading {logfile}", end="")
        if logfile.endswith(".gz"):
            with gzip.open(logfile, "rt", encoding="utf-8") as f:
                result = f.readlines()
        else:
            with open(logfile, "r", encoding="utf-8") as f:
                result = f.readlines()

    elapsed = time.time() - start
    verbose(f" ({len(result)} lines in {elapsed:.2f} seconds)")
    return result


def get_remote_contents(logfile: str, host: str) -> List[str]:
    """
    Get the lines in logfile on host. If logfile ends in .gz, decompress it.
    """
    cat_cmd = f"zcat" if logfile.endswith(".gz") else "cat"
    cmd = f"sudo ssh -o StrictHostKeyChecking=no -i ~apt-automation/.ssh/apt_automation.pem apt-automation@{host} sudo {cat_cmd} {logfile}"
    # Run the command and get the output
    stdout = subprocess.check_output(
        cmd, shell=True, encoding="utf-8", stderr=subprocess.DEVNULL
    )

    return stdout.splitlines()


def nginx_time_to_time_t(nginx_time: str) -> int:
    """
    Convert an nginx time string to a time_t.
    """
    when = time.strptime(nginx_time, "%d/%b/%Y:%H:%M:%S %z")
    return int(time.mktime(when) - when.tm_gmtoff)


def iso_8601_to_time_t(iso_8601: str) -> int:
    """
    Convert an ISO 8601 timestamp to a time_t.
    """
    if iso_8601.endswith("Z"):
        iso_8601 = f"{iso_8601[:-1]}+0000"
    when = time.strptime(iso_8601, "%Y-%m-%dT%H:%M:%S%z")
    return int(time.mktime(when) - when.tm_gmtoff)


def time_t_to_iso_8601_UTC(timet: int) -> str:
    """
    Convert a time_t to an ISO 8601 timestamp in UTC.
    """
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(timet))


def summarize_file(logfile: str, start_time: int = 0) -> Dict[str, Any]:
    """
    Compute a summary for a single nginx log file.

    Return the following information in a dict:
        status_counts: Counter of HTTP status codes
        per_status_counts: Dict of counters of HTTP status codes per URI (except status 200 and 304; the 404 URIs are filtered)
        unparseable: Set of unparseable log line
        elapsed_time: Time elapsed from first log entry (no earlier than start_time) to last log entry
        start and end times observed within the log file
        missing files (404s that exist in the local mirror, if the log file is local)
    """
    #  log_format noip 'redacted - $remote_user [$time_local] '  '"$request" $status $body_bytes_sent $request_time '  '"$http_referer" "$http_user_agent"';
    log_regex = re.compile(
        r"""^(?P<remote_addr>\S+) - (?P<remote_user>\S+) \[(?P<time_local>.+)\] "(?P<method>\S+) (?P<uri>\S+) +(?P<protocol>\S+)" (?P<status>\d+) (?P<body_bytes_sent>\d+) (?P<request_time>\d+\.\d+) "(?P<http_referer>.*)" "(?P<http_user_agent>.*)".*$"""
    )
    ignore_404 = re.compile(
        r"(/clamav/.*\.(cld|cdiff)$|.+/media\.1/media$|.+/Packages\.(bz2|zst|xz|lzma)$|.+/aplinux-prod/|.+/by-hash/|.+/repodata/.*\.sqlite\.bz2$|.+/main/i18n/Translation|/ubuntu/16\.04/.*/xenial|/ubuntu/18\.04/.*/(xenial|focal|jammy)/|/ubuntu/20\.04/.*/(bionic|jammy)/|.+-(primary|filelists)(\.xml)?\.gz$)"
    )
    force_404 = re.compile(r"(.+\.(cld|cdiff)$|.+/media\.1/media$|.+/aplinux-prod/)")

    per_status_counts = defaultdict(Counter)
    status_counts = Counter()
    earliest_time: Optional[int] = None
    end_time: Optional[int] = None
    unparseable = set()
    unforced_404s = Counter()
    forced_404_count = 0
    start = time.time()

    for line in get_contents(logfile):
        m = log_regex.match(line)
        if m:
            # convert time string to a time_t
            timet = nginx_time_to_time_t(m.group("time_local"))
            if timet < start_time:
                continue
            if not earliest_time:
                earliest_time = timet
            end_time = timet
            status = m.group("status")
            uri = m.group("uri")
            status_counts[status] += 1
            if status not in ("200", "304", "404") or (
                status == "404" and not ignore_404.match(uri)
            ):
                per_status_counts[status][uri] += 1
            if status == "404":
                if force_404.match(uri):
                    forced_404_count += 1
                else:
                    unforced_404s[uri] += 1
        else:
            unparseable.add(line)

    results = {
        "logfile": logfile,
        "elapsed_time": end_time - earliest_time if earliest_time else 0,
        "compute_time": time.time() - start,
    }
    if earliest_time:
        results["first_entry"] = time_t_to_iso_8601_UTC(earliest_time)
        results["first_entry_time_t"] = earliest_time
        results["last_entry"] = time_t_to_iso_8601_UTC(end_time)
        results["last_entry_time_t"] = end_time
    results["status_counts"] = Counter(
        {status: status_counts[status] for status in sorted(status_counts.keys())}
    )
    results["forced_404_count"] = forced_404_count
    results["top_404s"] = unforced_404s.most_common(20)
    if ":" not in logfile:
        results["missing_files"] = sorted(find_missing_files(per_status_counts))
    results["per_status_counts"] = per_status_counts
    results["unparseable"] = list(unparseable)

    return results


def find_missing_files(per_status_counts: Dict[str, Counter]) -> List[str]:
    """Find files that are 404s but exist in the vCurrent local mirror.

    Doesn't work for remote files, since there's no convenient local mirror to check.
    Note: Path("/var/www/html") / "/foo" is actually "/foo", not "/var/www/html/foo".
    """
    results = []
    www_root = Path("/var/www/html")
    for uri in per_status_counts["404"]:
        if ".." not in uri:
            try:
                target = www_root / uri[1:] if uri.startswith("/") else www_root / uri
                if target.exists():
                    results.append(uri)
            except Exception:
                verbose(f"Exception when checking for existence of /var/www/html/{uri}")
                pass
    return results


def filter_counts_by_status(
    per_status_counts: Dict[str, Counter], status: str, filter: Optional[Pattern]
) -> Counter:
    raw = per_status_counts[status]
    if filter is None:
        return raw
    else:
        return Counter(
            {uri: count for uri, count in raw.items() if not filter.match(uri)}
        )


def display_results(
    args: argparse.Namespace,
    filename: str,
    results: Dict[str, Any],
):
    """
    Display a PMC-relevant summary of a single log file

    Args:
        filename: Name of the log file
        results: The results dict from summarize_file()
    """
    time_span = results["elapsed_time"]
    compute_time = results["compute_time"]
    print(
        f"{filename} spans {time_span} seconds processed in {compute_time:.2f} seconds"
    )
    lines = []
    for status, count in results["status_counts"].items():
        lines.append(f"{status}: {count} ({count / time_span:.2f}/s)")
    print("\n".join(sorted(lines)))

    if "missing_files" not in results:
        print("Suppressed missing file checks for remote log file")
    else:
        missing_files = results["missing_files"]
        if len(missing_files):
            nl = "\n"
            print(f"Missing files:{nl}{nl.join(missing_files)}")
        else:
            print("No 404s have existing files under /var/www/html")

    print(f"\nForced 404s: {results['forced_404_count']}")
    print("Top unforced 404s:")
    print("\n".join(f"{uri}: {count}" for uri, count in results["top_404s"]), "\n")

    if args.view_status:
        uri_counts = results[f"status_{args.view_status}"]
        print(f"{len(uri_counts)} status {args.view_status} URIs")
        lines = [f"{uri}: {count}" for uri, count in uri_counts.items()]
        print("\n".join(sorted(lines)))

    if args.view_unparseable:
        print(f"{len(results['unparseable'])} unparseable lines")
        print("\n".join(sorted(results["unparseable"])), "\n")

    if args.view_404:
        entries = results["per_status_counts"]["404"]
        print(f"Filtered 404 URIs:")
        print(
            "\n".join(f"{uri}: {entries[uri]}" for uri in sorted(entries.keys())), "\n"
        )


def make_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="log-analyze", description="Analyze PMC nginx logs."
    )
    parser.add_argument(
        "logfiles",
        nargs="*",
        help="Log files to analyze. Format is [host:]name where host specifies an ssh-accessible server and name is either absolute or relative to /var/log/nginx. Default is 'access.log'.",
    )
    parser.add_argument(
        "--start-time",
        type=str,
        default="1970-01-01T00:00:00+0000",
        help="ISO 8601 datetime before which log records are ignored.",
    )
    parser.add_argument(
        "--view-status",
        type=str,
        default=None,
        help="View requested URIs receiving a specific status code.",
    )
    parser.add_argument(
        "--view-filter",
        type=str,
        default=None,
        help="Ignore URIs (returned by --view-status) matching this regex.",
    )
    parser.add_argument("--view-unparseable", action="store_true")
    parser.add_argument("--view-404", action="store_true")
    parser.add_argument("--json", action="store_true")
    parser.add_argument("--verbose", action="store_true")
    parser.add_argument("-v", "--version", action="version", version=VERSION)
    return parser


def main():
    global show_verbose
    args = make_parser().parse_args()
    show_verbose = args.verbose
    filenames = args.logfiles
    if not filenames:
        filenames = ["access.log"]

    """Analyze PMC nginx logs."""
    start_time = iso_8601_to_time_t(args.start_time)
    total_counts = Counter()
    all_404s = Counter()
    summaries = []
    for filename in args.logfiles:
        results = summarize_file(filename, start_time)

        if args.view_status:
            results[f"status_{args.view_status}"] = filter_counts_by_status(
                results["per_status_counts"], args.view_status, args.view_filter
            )

        if args.json:
            summaries.append(results)
        else:
            display_results(args, filename, results)
        total_counts = total_counts + results["status_counts"]
        all_404s = all_404s + results["per_status_counts"]["404"]

    num_requests = sum(total_counts.values())
    num_files = len(args.logfiles)
    if args.json:
        json_results = {
            "hostname": os.uname().nodename,
            "num_requests": num_requests,
            "total_counts": total_counts,
            "summaries": summaries,
        }
        print(json.dumps(json_results, indent=2))
    elif len(args.logfiles) > 1:
        files = "files " if num_files > 1 else "file"
        print(f"Total of {num_requests} records in {num_files} {files}")
        lines = []
        for status, count in total_counts.items():
            lines.append(f"{status}: {count} ({100. * count / num_requests:.2f}%)")
        print("\n".join(sorted(lines)))

        if args.view_404:
            print(f"{len(all_404s)} 404s")
            lines = [f"{uri}: {all_404s[uri]}" for uri in sorted(all_404s.keys())]
            print("\n".join(sorted(lines)))


if __name__ == "__main__":
    main()
